{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Titanic Ship Passenger Survival\n",
    " Problem class: Supervised Classification\n",
    " \n",
    " Problem dataset link: \n",
    " \n",
    " Problem description: \n",
    " \n",
    "    Dataset contains information about the passengers travelling on the famous ship\n",
    "    Titanic on the fateful sail that saw it's sinking\n",
    " \n",
    " Problem Task:\n",
    "     Have to predict who will servive after ship sink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature or column description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COLUMN DESCRIPTIONS:\n",
    "* pclass Passenger Class ( 1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "* survival Survival ( 0 = No; 1 = Yes)\n",
    "* name Name\n",
    "* sex Sex\n",
    "* age Age\n",
    "* sibsp Number of Siblings/Spouses Aboard\n",
    "* parch Number of Parents/Children Aboard\n",
    "* ticket Ticket Number\n",
    "* fare Passenger Fare\n",
    "* cabin Cabin\n",
    "* embarked Port of Embarkation ( C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "* boat Lifeboat\n",
    "* body Body Identification Number\n",
    "* home. dest Home/Destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_or_url = \"dataset/titanic3.csv\"\n",
    "\n",
    "# pd.read_csv(filepath, sep=', ', dtype=None, header=None, skiprows=None,index_col=None, skip_blank_lines=TRUE, na_filter=TRUE)\n",
    "\n",
    "# read_csv method can also work for text and url file path\n",
    "data = pd.read_csv(dataset_path_or_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read_csv method can also work for text and url file path\n",
    "# test_text_dataset = \"dataset/Customer Churn Model.txt\"\n",
    "# data_text = pd.read_csv(test_text_dataset)\n",
    "# data_text.columns.values\n",
    "## read_csv method can also work for text and url file path\n",
    "# test_csv_dataset = \"dataset/Customer Churn Model.csv\"\n",
    "# data_csv_columns = pd.read_csv(test_csv_dataset)\n",
    "# data_csv_column_list = data_csv_columns['Column_Names'].tolist()\n",
    "\n",
    "# test_text_dataset = \"dataset/Customer Churn Model.txt\"\n",
    "# data_text = pd.read_csv(test_text_dataset, header=None, names=data_csv_column_list)\n",
    "\n",
    "# data_text.columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 3332 rows and 21 columns\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "\"\"\"pandas is a very robust and comprehensive library to read, explore, and manipulate a\n",
    "dataset. But, it might not give an optimal performance with very big datasets as it reads\n",
    "the entire dataset, all at once, and blocks the majority of computer memory. Instead, you\n",
    "can try one of the Python’s file handling methods—open\"\"\"\n",
    "\n",
    "\n",
    "data=open('dataset/Customer Churn Model.txt','r')\n",
    "cols = data.__next__().strip().split(',')\n",
    "no_cols = len(data.__next__().strip().split(','))\n",
    "\n",
    "counter=0\n",
    "main_dict={}\n",
    "for col in cols:\n",
    "    main_dict[col]=[]\n",
    "\n",
    "\n",
    "for line in data:\n",
    "    values = line.strip().split(',')\n",
    "    for i in range(len(cols)):\n",
    "        main_dict[cols[i]].append(values[i])\n",
    "    counter += 1\n",
    "\n",
    "print(\"The dataset has %d rows and %d columns\" % (counter,no_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
